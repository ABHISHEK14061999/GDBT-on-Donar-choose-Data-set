{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wqapAXjsc24"
   },
   "source": [
    "# Assignment 9: GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8u5NhbnEsc2-"
   },
   "source": [
    "#### Response Coding: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIFWmwHKsc3A"
   },
   "source": [
    "<img src='http://i.imgur.com/TufZptV.jpg' width=700px>\n",
    "\n",
    "> The response tabel is built only on train dataset.\n",
    "> For a category which is not there in train data and present in test data, we will encode them with default values\n",
    "Ex: in our test data if have State: D then we encode it as [0.5, 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KwfGufDsc3D"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply GBDT on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical(instead of one hot encoding, try <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/'>response coding</a>: use probability values), numerical features + project_title(TFIDF)+  preprocessed_eassay (TFIDF)+sentiment Score of eassay(check the bellow example, include all 4 values as 4 features)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical(instead of one hot encoding, try <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/'>response coding</a>: use probability values), numerical features + project_title(TFIDF W2V)+  preprocessed_eassay (TFIDF W2V)</li>        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning (Consider any two hyper parameters)</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation/simple cross validation data</li>\n",
    "    <li>use gridsearch cv or randomsearch cv or you can write your own for loops to do this task</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>n_estimators</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n",
    "            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>n_estimators</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n",
    "    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "            </ul>\n",
    "    <br>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iffa2_WGsc3H",
    "outputId": "151fc2d5-3bc1-4206-9bd2-000dbf662b31"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for_sentiment = 'a person is a person no matter how small dr seuss i teach the smallest students with the biggest enthusiasm \\\n",
    "for learning my students learn in many different ways using all of our senses and multiple intelligences i use a wide range\\\n",
    "of techniques to help all my students succeed students in my class come from a variety of different backgrounds which makes\\\n",
    "for wonderful sharing of experiences and cultures including native americans our school is a caring community of successful \\\n",
    "learners which can be seen through collaborative student project based learning in and out of the classroom kindergarteners \\\n",
    "in my class love to work with hands on materials and have many different opportunities to practice a skill before it is\\\n",
    "mastered having the social skills to work cooperatively with friends is a crucial aspect of the kindergarten curriculum\\\n",
    "montana is the perfect place to learn about agriculture and nutrition my students love to role play in our pretend kitchen\\\n",
    "in the early childhood classroom i have had several kids ask me can we try cooking with real food i will take their idea \\\n",
    "and create common core cooking lessons where we learn important math and writing concepts while cooking delicious healthy \\\n",
    "food for snack time my students will have a grounded appreciation for the work that went into making the food and knowledge \\\n",
    "of where the ingredients came from as well as how it is healthy for their bodies this project would expand our learning of \\\n",
    "nutrition and agricultural cooking recipes by having us peel our own apples to make homemade applesauce make our own bread \\\n",
    "and mix up healthy plants from our classroom garden in the spring we will also create our own cookbooks to be printed and \\\n",
    "shared with families students will gain math and literature skills as well as a life long enjoyment for healthy cooking \\\n",
    "nannan'\n",
    "ss = sid.polarity_scores(for_sentiment)\n",
    "\n",
    "for k in ss:\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "\n",
    "# we can use these 4 things as features/attributes (neg, neu, pos, compound)\n",
    "# neg: 0.0, neu: 0.753, pos: 0.247, compound: 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOyTl7oKsc3W"
   },
   "source": [
    "<h1>1. GBDT (xgboost/lightgbm) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWge-b2Zsc3Z"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDZFu6-usc3d"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#from plotly import plotly\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "offline.init_notebook_mode()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Abhishek V/Downloads/preprocessed_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = data['project_is_approved'].values\n",
    "X = data.drop(['project_is_approved'], axis=1)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lAMFKFcsc3j"
   },
   "source": [
    "<h2>1.2 Splitting data into Train and cross validation(or test): Stratified Sampling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODMijuuNsc3l"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1.2.1 Splitting data into Train and cross validation(or test): Stratified Sampling [ ]\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.33, stratify=y)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsALgl5Asc3u"
   },
   "source": [
    "<h2>1.3 Make Data Model Ready: encoding eassay, and project_title</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A_85jbWsc3v"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_cv.shape, y_cv.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,4), max_features=5000)\n",
    "\n",
    "# encoding eassay\n",
    "vectorizer.fit(X_train['essay'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_essay_tfidf = vectorizer.transform(X_train['essay'].values)\n",
    "X_cv_essay_tfidf = vectorizer.transform(X_cv['essay'].values)\n",
    "X_test_essay_tfidf = vectorizer.transform(X_test['essay'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_essay_tfidf.shape, y_train.shape)\n",
    "print(X_cv_essay_tfidf.shape, y_cv.shape)\n",
    "print(X_test_essay_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3Vxxv2Hsc31"
   },
   "source": [
    "<h2>1.4 Make Data Model Ready: encoding numerical, categorical features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RAtihVPqsc33"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding \n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gv_fea_dict(alpha, feature, df):\n",
    "    \n",
    "    value_count = X_train[feature].value_counts()\n",
    "    \n",
    "    # gv_dict : Gene Variation Dict, which contains the probability array for each gene/variation\n",
    "    gv_dict = dict()\n",
    "    \n",
    "    # denominator will contain the number of time that particular feature occured in whole data\n",
    "    for i, denominator in value_count.items():\n",
    "        # vec will contain (p(yi==1/Gi) probability of gene/variation belongs to perticular class\n",
    "        # vec is 9 diamensional vector\n",
    "        vec = []\n",
    "        for k in range(0,2):\n",
    "            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n",
    "            \n",
    "            # cls_cnt.shape[0] will return the number of rows\n",
    "\n",
    "            cls_cnt = X_train.loc[(X_train['project_is_approved']==k) & (X_train[feature]==i)]\n",
    "            \n",
    "            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n",
    "            vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n",
    "\n",
    "        # we are adding the gene/variation to the dict as key and vec as value\n",
    "        gv_dict[i]=vec\n",
    "    return gv_dict\n",
    "\n",
    "# Get Gene variation feature\n",
    "def get_gv_feature(alpha, feature, df):\n",
    "    # print(gv_dict)\n",
    "    \n",
    "    gv_dict = get_gv_fea_dict(alpha, feature, df)\n",
    "    # value_count is similar in get_gv_fea_dict\n",
    "    value_count = X_train[feature].value_counts()\n",
    "    \n",
    "    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n",
    "    gv_fea = []\n",
    "    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n",
    "    # if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n",
    "    for index, row in df.iterrows():\n",
    "        if row[feature] in dict(value_count).keys():\n",
    "            gv_fea.append(gv_dict[row[feature]])\n",
    "        else:\n",
    "            gv_fea.append([.5,.5])\n",
    "#             gv_fea.append([-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
    "    return gv_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4.1 Response encoding categorical features: School State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response-coding of the School State feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train state feature\n",
    "X_train_state_feature_responseCoding = np.array(get_gv_feature(alpha, \"school_state\", X_train))\n",
    "# test state feature\n",
    "X_test_state_feature_responseCoding = np.array(get_gv_feature(alpha, \"school_state\", X_test))\n",
    "# cross validation state feature\n",
    "X_cv_state_feature_responseCoding = np.array(get_gv_feature(alpha, \"school_state\", X_cv))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_state_feature_responseCoding.shape, y_train.shape)\n",
    "print(X_cv_state_feature_responseCoding.shape, y_cv.shape)\n",
    "print(X_test_state_feature_responseCoding.shape, y_test.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4.2 Response encoding categorical features: teacher_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response-coding of the teacher_prefix feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train state feature\n",
    "X_train_teacher_prefix_feature_responseCoding = np.array(get_gv_feature(alpha, \"teacher_prefix\", X_train))\n",
    "# test state feature\n",
    "X_test_teacher_prefix_feature_responseCoding = np.array(get_gv_feature(alpha, \"teacher_prefix\", X_test))\n",
    "# cross validation state feature\n",
    "X_cv_teacher_prefix_feature_responseCoding = np.array(get_gv_feature(alpha, \"teacher_prefix\", X_cv))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_teacher_prefix_feature_responseCoding.shape, y_train.shape)\n",
    "print(X_cv_teacher_prefix_feature_responseCoding.shape, y_cv.shape)\n",
    "print(X_test_teacher_prefix_feature_responseCoding.shape, y_test.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4.3 Response encoding categorical features: project_grade_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response-coding of the project_grade_category feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train state feature\n",
    "X_train_project_grade_category_feature_responseCoding = np.array(get_gv_feature(alpha, \"project_grade_category\", X_train))\n",
    "# test state feature\n",
    "X_test_project_grade_category_feature_responseCoding = np.array(get_gv_feature(alpha, \"project_grade_category\", X_test))\n",
    "# cross validation state feature\n",
    "X_cv_project_grade_category_feature_responseCoding = np.array(get_gv_feature(alpha, \"project_grade_category\", X_cv))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_project_grade_category_feature_responseCoding.shape, y_train.shape)\n",
    "print(X_cv_project_grade_category_feature_responseCoding.shape, y_cv.shape)\n",
    "print(X_test_project_grade_category_feature_responseCoding.shape, y_test.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.4 Response encoding categorical features: clean_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#response-coding of the clean_categories feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train state feature\n",
    "X_train_clean_categories_feature_responseCoding = np.array(get_gv_feature(alpha, \"clean_categories\", X_train))\n",
    "# test state feature\n",
    "X_test_clean_categories_feature_responseCoding = np.array(get_gv_feature(alpha, \"clean_categories\", X_test))\n",
    "# cross validation state feature\n",
    "X_cv_clean_categories_feature_responseCoding = np.array(get_gv_feature(alpha, \"clean_categories\", X_cv))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_clean_categories_feature_responseCoding.shape, y_train.shape)\n",
    "print(X_cv_clean_categories_feature_responseCoding.shape, y_cv.shape)\n",
    "print(X_test_clean_categories_feature_responseCoding.shape, y_test.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.5 Response encoding categorical features: clean_subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#response-coding of the clean_subcategories feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train state feature\n",
    "X_train_clean_subcategories_feature_responseCoding = np.array(get_gv_feature(alpha, \"clean_subcategories\", X_train))\n",
    "# test state feature\n",
    "X_test_clean_subcategories_feature_responseCoding = np.array(get_gv_feature(alpha, \"clean_subcategories\", X_test))\n",
    "# cross validation state feature\n",
    "X_cv_clean_subcategories_feature_responseCoding = np.array(get_gv_feature(alpha, \"clean_subcategories\", X_cv))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_clean_subcategories_feature_responseCoding.shape, y_train.shape)\n",
    "print(X_cv_clean_subcategories_feature_responseCoding.shape, y_cv.shape)\n",
    "print(X_test_clean_subcategories_feature_responseCoding.shape, y_test.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.6 encoding numerical features: Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# normalizer.fit(X_train['price'].values)\n",
    "# this will rise an error Expected 2D array, got 1D array instead: \n",
    "# array=[105.22 215.96  96.01 ... 368.98  80.53 709.67].\n",
    "# Reshape your data either using \n",
    "# array.reshape(-1, 1) if your data has a single feature \n",
    "# array.reshape(1, -1)  if it contains a single sample.\n",
    "#normalizer.fit(X_train['price'].values.reshape(-1,1))\n",
    "normalizer.fit(X_train['price'].values.reshape(1,-1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(1,-1))\n",
    "X_cv_price_norm = normalizer.transform(X_cv['price'].values.reshape(1,-1))\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(1,-1))\n",
    "\n",
    "X_train_price_norm =X_train_price_norm.reshape(-1,1)\n",
    "X_cv_price_norm = X_cv_price_norm.reshape(-1,1)\n",
    "X_test_price_norm = X_test_price_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "#print(X_train_price_norm_1.shape, y_train.shape)\n",
    "print(X_train_price_norm.shape, y_train.shape)\n",
    "print(X_cv_price_norm.shape, y_cv.shape)\n",
    "print(X_test_price_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4.7 encoding numerical features: teacher_number_of_previously_posted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# normalizer.fit(X_train['price'].values)\n",
    "# this will rise an error Expected 2D array, got 1D array instead: \n",
    "# array=[105.22 215.96  96.01 ... 368.98  80.53 709.67].\n",
    "# Reshape your data either using \n",
    "# array.reshape(-1, 1) if your data has a single feature \n",
    "# array.reshape(1, -1)  if it contains a single sample.\n",
    "normalizer.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "\n",
    "X_train_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "X_cv_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "X_test_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "\n",
    "X_train_teacher_number_of_previously_posted_projects_norm = X_train_teacher_number_of_previously_posted_projects_norm.reshape(-1,1)\n",
    "X_cv_teacher_number_of_previously_posted_projects_norm    = X_cv_teacher_number_of_previously_posted_projects_norm.reshape(-1,1)\n",
    "X_test_teacher_number_of_previously_posted_projects_norm  = X_test_teacher_number_of_previously_posted_projects_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_teacher_number_of_previously_posted_projects_norm.shape, y_train.shape)\n",
    "print(X_cv_teacher_number_of_previously_posted_projects_norm.shape, y_cv.shape)\n",
    "print(X_test_teacher_number_of_previously_posted_projects_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "#X_train_teacher_number_of_previously_posted_projects_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.8 Score of Eassay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_essay_fea_score(feature, df):\n",
    "   \n",
    "    #value_count = X_train[feature].value_counts()\n",
    "    vec =[]\n",
    "   \n",
    "    for index, row in df.iterrows():\n",
    "      X_train_essay_score = sid.polarity_scores(row[feature])\n",
    "      vec.append(list(X_train_essay_score.values()))\n",
    "      \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_cv.shape, y_cv.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "X_train_essay_feature_score = np.array(get_essay_fea_score(\"essay\", X_train))\n",
    "X_cv_essay_feature_score = np.array(get_essay_fea_score(\"essay\", X_cv))\n",
    "X_test_essay_feature_score = np.array(get_essay_fea_score(\"essay\", X_test))\n",
    "\n",
    "#X_train_essay_feature_score = X_train_essay_feature_score.reshape(-1,1)\n",
    "#X_cv_essay_feature_score = X_cv_essay_feature_score.reshape(-1,1)\n",
    "#X_test_essay_feature_score = X_test_essay_feature_score.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_essay_feature_score.shape, y_train.shape)\n",
    "print(X_cv_essay_feature_score.shape, y_cv.shape)\n",
    "print(X_test_essay_feature_score.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinating all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "X_tr = hstack((X_train_essay_tfidf, X_train_state_feature_responseCoding, X_train_teacher_prefix_feature_responseCoding, X_train_project_grade_category_feature_responseCoding,X_train_clean_categories_feature_responseCoding,X_train_clean_subcategories_feature_responseCoding, X_train_price_norm,X_train_teacher_number_of_previously_posted_projects_norm,X_train_essay_feature_score)).tocsr()\n",
    "X_cr = hstack((X_cv_essay_tfidf, X_cv_state_feature_responseCoding, X_cv_teacher_prefix_feature_responseCoding, X_cv_project_grade_category_feature_responseCoding, X_cv_clean_categories_feature_responseCoding,X_cv_clean_subcategories_feature_responseCoding,X_cv_price_norm,X_cv_teacher_number_of_previously_posted_projects_norm,X_cv_essay_feature_score)).tocsr()\n",
    "X_te = hstack((X_test_essay_tfidf, X_test_state_feature_responseCoding, X_test_teacher_prefix_feature_responseCoding, X_test_project_grade_category_feature_responseCoding,X_test_clean_categories_feature_responseCoding,X_test_clean_subcategories_feature_responseCoding, X_test_price_norm,X_test_teacher_number_of_previously_posted_projects_norm,X_test_essay_feature_score)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr.shape, y_train.shape)\n",
    "print(X_cr.shape, y_cv.shape)\n",
    "print(X_te.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5wDFj17sc3-"
   },
   "source": [
    "<h2>1.5 Appling Models on different kind of featurization as mentioned in the instructions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5tWox1jsc3_"
   },
   "source": [
    "<br>Apply GBDT on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARWulYUwsc4B"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5.1 Appling DT: TFIDF featurization\n",
    "\n",
    "1.5.1.1 Hyper parameter Tuning\n",
    "\n",
    "1.5.1.1.2 - Random Search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "#k =[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "K=[1, 5, 10, 50]\n",
    "alpha = [100,200,500,1000,2000]\n",
    "#min_samples_split= [5, 10, 100, 500]\n",
    "#C = uniform(0.00001,10000)\n",
    "#parameters = {'alpha':uniform(0.00001,10000)}\n",
    "parameters = dict(max_depth=K,n_estimators=alpha)\n",
    "clf = RandomizedSearchCV(GradientBoostingClassifier(), parameters, cv=3, scoring='roc_auc',return_train_score=True)\n",
    "clf.fit(X_tr, y_train)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "#results = results.sort_values(['param_alpha'])\n",
    "results = results.sort_values(['param_max_depth'])\n",
    "train_auc= results['mean_train_score']\n",
    "#train_auc_std= results['std_train_score']\n",
    "cv_auc = results['mean_test_score'] \n",
    "#cv_auc_std= results['std_test_score']\n",
    "depth =  results['param_max_depth']\n",
    "estimators = results['param_n_estimators']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # AUC Matrix with Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df1 = results[['mean_train_score','param_max_depth','param_n_estimators']]\n",
    "#df1.head()\n",
    "heatmap1_data = pd.pivot_table(df1, values='mean_train_score', \n",
    "                     index=['param_n_estimators'], \n",
    "                     columns='param_max_depth')\n",
    "sns.heatmap(heatmap1_data, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AUC Matrix with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = results[['mean_test_score','param_max_depth','param_n_estimators']]\n",
    "heatmap2_data = pd.pivot_table(df2, values='mean_test_score', \n",
    "                     index=['param_n_estimators'], \n",
    "                     columns='param_max_depth')\n",
    "sns.heatmap(heatmap2_data, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.5.1.2 Testing the performance of the model on test data, plotting ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the error plot we choose K such that, we will have maximum AUC on cv data and gap between the train and cv is less\n",
    "# Note: based on the method you use you might get different hyperparameter values as best one\n",
    "# so, you choose according to the method you choose, you use gridsearch if you are having more computing power and note it will take more time\n",
    "# if you increase the cv values in the GridSearchCV you will get more rebust results.\n",
    "\n",
    "#here we are choosing the best_k based on forloop results\n",
    "best_depth = 1\n",
    "best_estimator = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(max_depth=best_depth,n_estimators=best_estimator)\n",
    "clf.fit(X_tr, y_train)\n",
    "# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "# not the predicted outputs\n",
    "\n",
    "#y_train_pred = batch_predict(clf, X_tr)\n",
    "y_train_pred = clf.predict(X_tr)    \n",
    "#y_test_pred = batch_predict(clf, X_te)\n",
    "y_test_pred = clf.predict(X_te)\n",
    "\n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"K: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are writing our own function for predict, with defined thresould\n",
    "# we will pick a threshold that will give the least fpr\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)), annot=True, ax = ax,fmt='g');\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)), annot=True, ax = ax,fmt='g');\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5.2 Appling Decision Tree: TFIDFW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "# make sure you have the glove_vectors file\n",
    "with open('C:/Users/Abhishek V/Downloads/glove_vectors (3)', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(X_train['essay'].values)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "tfidf_words = set(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_w2v(df,feature):\n",
    "  \n",
    "  essay_tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "  for sentence in tqdm(df[feature].values): # for each review/sentence\n",
    "      vector = np.zeros(300) # as word vectors are of zero length\n",
    "      tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "      for word in sentence.split(): # for each word in a review/sentence\n",
    "          if (word in glove_words) and (word in tfidf_words):\n",
    "              vec = model[word] # getting the vector for each word\n",
    "              # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "              tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "              vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "              tf_idf_weight += tf_idf\n",
    "      if tf_idf_weight != 0:\n",
    "          vector /= tf_idf_weight\n",
    "      essay_tfidf_w2v_vectors.append(vector)\n",
    "      \n",
    "  return essay_tfidf_w2v_vectors\n",
    "\n",
    "#X_train_essay_tfidf_w2v_vectors_2 = np.array(tfidf_w2v(X_train,'essay'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_essay_tfidf_w2v_vectors = np.array(tfidf_w2v(X_train,'essay'))\n",
    "X_cv_essay_tfidf_w2v_vectors    = np.array(tfidf_w2v(X_cv,'essay'))\n",
    "X_test_essay_tfidf_w2v_vectors    = np.array(tfidf_w2v(X_test,'essay'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_cv.shape, y_cv.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_essay_tfidf_w2v_vectors.shape, y_train.shape)\n",
    "print(X_cv_essay_tfidf_w2v_vectors.shape, y_cv.shape)\n",
    "print(X_test_essay_tfidf_w2v_vectors.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.5.2.2 Concatinating all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "X_train_essay_tfidf_w2v_vectors = sparse.csr_matrix(X_train_essay_tfidf_w2v_vectors)\n",
    "X_cv_essay_tfidf_w2v_vectors = sparse.csr_matrix(X_cv_essay_tfidf_w2v_vectors)\n",
    "X_test_essay_tfidf_w2v_vectors = sparse.csr_matrix(X_test_essay_tfidf_w2v_vectors)\n",
    "X_tr = hstack((X_train_essay_tfidf_w2v_vectors, X_train_state_feature_responseCoding, X_train_teacher_prefix_feature_responseCoding, X_train_project_grade_category_feature_responseCoding,X_train_clean_categories_feature_responseCoding,X_train_clean_subcategories_feature_responseCoding, X_train_price_norm,X_train_teacher_number_of_previously_posted_projects_norm,X_train_essay_feature_score)).tocsr()\n",
    "X_cr = hstack((X_cv_essay_tfidf_w2v_vectors, X_cv_state_feature_responseCoding, X_cv_teacher_prefix_feature_responseCoding, X_cv_project_grade_category_feature_responseCoding, X_cv_clean_categories_feature_responseCoding,X_cv_clean_subcategories_feature_responseCoding,X_cv_price_norm,X_cv_teacher_number_of_previously_posted_projects_norm,X_cv_essay_feature_score)).tocsr()\n",
    "X_te = hstack((X_test_essay_tfidf_w2v_vectors, X_test_state_feature_responseCoding, X_test_teacher_prefix_feature_responseCoding, X_test_project_grade_category_feature_responseCoding,X_test_clean_categories_feature_responseCoding,X_test_clean_subcategories_feature_responseCoding, X_test_price_norm,X_test_teacher_number_of_previously_posted_projects_norm,X_test_essay_feature_score)).tocsr()\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr.shape, y_train.shape)\n",
    "print(X_cr.shape, y_cv.shape)\n",
    "print(X_te.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.5.3.1 Hyper parameter Tuning\n",
    "\n",
    "1.5.3.1.1 Method 1: Simple for loop (if you are having memory limitations use this)\n",
    "\n",
    "1.5.3.1.2 Method 2:-RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "K=[1, 5, 10, 50]\n",
    "alpha = [100,200,500,1000,2000]\n",
    "#min_samples_split= [5, 10, 100, 500]\n",
    "#C = uniform(0.00001,10000)\n",
    "#parameters = {'alpha':uniform(0.00001,10000)}\n",
    "parameters = dict(max_depth=K,n_estimators=alpha)\n",
    "\n",
    "clf = RandomizedSearchCV(GradientBoostingClassifier(), parameters, cv=3, scoring='roc_auc',return_train_score=True)\n",
    "clf.fit(X_tr, y_train)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "#results = results.sort_values(['param_alpha'])\n",
    "results = results.sort_values(['param_max_depth'])\n",
    "train_auc= results['mean_train_score']\n",
    "#train_auc_std= results['std_train_score']\n",
    "cv_auc = results['mean_test_score'] \n",
    "#cv_auc_std= results['std_test_score']\n",
    "depth =  results['param_max_depth']\n",
    "estimators = results['param_n_estimators']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC Matrix with Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df1 = results[['mean_train_score','param_max_depth','param_n_estimators']]\n",
    "#df1.head()\n",
    "heatmap1_data = pd.pivot_table(df1, values='mean_train_score', \n",
    "                     index=['param_n_estimators'], \n",
    "                     columns='param_max_depth')\n",
    "sns.heatmap(heatmap1_data, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUC Matrix with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = results[['mean_test_score','param_max_depth','param_n_estimators']]\n",
    "heatmap2_data = pd.pivot_table(df2, values='mean_test_score', \n",
    "                     index=['param_n_estimators'], \n",
    "                     columns='param_max_depth')\n",
    "sns.heatmap(heatmap2_data, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.5.3.2 Testing the performance of the model on test data, plotting ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth =1\n",
    "best_estimator = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(max_depth=best_depth,n_estimators=best_estimator)\n",
    "clf.fit(X_tr, y_train)\n",
    "# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "# not the predicted outputs\n",
    "\n",
    "#y_train_pred = batch_predict(clf, X_tr)\n",
    "y_train_pred = clf.predict(X_tr)    \n",
    "#y_test_pred = batch_predict(clf, X_te)\n",
    "y_test_pred = clf.predict(X_te)\n",
    "\n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"K: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"ERROR PLOTS\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.Representation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)), annot=True, ax = ax,fmt='g');\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)), annot=True, ax = ax,fmt='g');\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Vectorizer\",\"Model\",\"HYPER PARAMETER-depth , n_estimators\",\"AUC\"]\n",
    "\n",
    "x.add_row([\"TFIDF\",\"GBDT-RandomSearch\",\"1,1000\",0.5103])\n",
    "x.add_row([\"TFIDF W2V\",\"GBDT-RandomSearch\",\"1,500\",0.504])\n",
    "\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11_Assignment_GBDT_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
